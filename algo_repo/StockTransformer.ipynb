{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Stock Market Transformer Model</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import TorchNormalizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_forecasting.models.base_model import BaseModel, BaseModelWithCovariates\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Data</h3>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": "                Open      High       Low     Close     Volume\nDate                                                         \n1980-12-12  0.100323  0.100759  0.100323  0.100323  469033600\n1980-12-15  0.095525  0.095525  0.095089  0.095089  175884800\n1980-12-16  0.088546  0.088546  0.088110  0.088110  105728000\n1980-12-17  0.090291  0.090727  0.090291  0.090291   86441600\n1980-12-18  0.092908  0.093345  0.092908  0.092908   73449600",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1980-12-12</th>\n      <td>0.100323</td>\n      <td>0.100759</td>\n      <td>0.100323</td>\n      <td>0.100323</td>\n      <td>469033600</td>\n    </tr>\n    <tr>\n      <th>1980-12-15</th>\n      <td>0.095525</td>\n      <td>0.095525</td>\n      <td>0.095089</td>\n      <td>0.095089</td>\n      <td>175884800</td>\n    </tr>\n    <tr>\n      <th>1980-12-16</th>\n      <td>0.088546</td>\n      <td>0.088546</td>\n      <td>0.088110</td>\n      <td>0.088110</td>\n      <td>105728000</td>\n    </tr>\n    <tr>\n      <th>1980-12-17</th>\n      <td>0.090291</td>\n      <td>0.090727</td>\n      <td>0.090291</td>\n      <td>0.090291</td>\n      <td>86441600</td>\n    </tr>\n    <tr>\n      <th>1980-12-18</th>\n      <td>0.092908</td>\n      <td>0.093345</td>\n      <td>0.092908</td>\n      <td>0.092908</td>\n      <td>73449600</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "data = yf.download(tickers=\"AAPL\", period='max', interval='1d', groupby='ticker', auto_adjust='True')\n",
    "data.head()\n",
    "\n",
    "#create random data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "   order         Date      Open      High       Low     Close     Volume  \\\n0      0  345427200.0  0.100323  0.100759  0.100323  0.100323  469033600   \n1      1  345686400.0  0.095525  0.095525  0.095089  0.095089  175884800   \n2      2  345772800.0  0.088546  0.088546  0.088110  0.088110  105728000   \n3      3  345859200.0  0.090291  0.090727  0.090291  0.090291   86441600   \n4      4  345945600.0  0.092908  0.093345  0.092908  0.092908   73449600   \n\n     Target  \n0  0.095089  \n1  0.088110  \n2  0.090291  \n3  0.092908  \n4  0.098578  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order</th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>345427200.0</td>\n      <td>0.100323</td>\n      <td>0.100759</td>\n      <td>0.100323</td>\n      <td>0.100323</td>\n      <td>469033600</td>\n      <td>0.095089</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>345686400.0</td>\n      <td>0.095525</td>\n      <td>0.095525</td>\n      <td>0.095089</td>\n      <td>0.095089</td>\n      <td>175884800</td>\n      <td>0.088110</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>345772800.0</td>\n      <td>0.088546</td>\n      <td>0.088546</td>\n      <td>0.088110</td>\n      <td>0.088110</td>\n      <td>105728000</td>\n      <td>0.090291</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>345859200.0</td>\n      <td>0.090291</td>\n      <td>0.090727</td>\n      <td>0.090291</td>\n      <td>0.090291</td>\n      <td>86441600</td>\n      <td>0.092908</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>345945600.0</td>\n      <td>0.092908</td>\n      <td>0.093345</td>\n      <td>0.092908</td>\n      <td>0.092908</td>\n      <td>73449600</td>\n      <td>0.098578</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.index = data.index.set_names([\"order\"])\n",
    "data.reset_index(inplace=True)#to keep up with order\n",
    "data['Target'] = data[\"Close\"].shift(-1)\n",
    "data[\"Date\"] = data[\"Date\"].apply(lambda x: x.value/10**9)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop([\"Target\"], axis=1)\n",
    "y = data[\"Target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=42, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 3.4543e+08, 1.0032e-01,  ..., 1.0032e-01, 1.0032e-01,\n         4.6903e+08],\n        [1.0000e+00, 3.4569e+08, 9.5525e-02,  ..., 9.5089e-02, 9.5089e-02,\n         1.7588e+08],\n        [2.0000e+00, 3.4577e+08, 8.8546e-02,  ..., 8.8110e-02, 8.8110e-02,\n         1.0573e+08],\n        ...,\n        [8.4250e+03, 1.3996e+09, 1.8702e+01,  ..., 1.8568e+01, 1.8734e+01,\n         2.9160e+08],\n        [8.4260e+03, 1.3999e+09, 1.8797e+01,  ..., 1.8794e+01, 1.8968e+01,\n         2.1321e+08],\n        [8.4270e+03, 1.3999e+09, 1.8941e+01,  ..., 1.8899e+01, 1.8997e+01,\n         1.5974e+08]])"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(X_train.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "(8428, 7)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    #must overwrite __getitem__, __len__, __\n",
    "    def __init__(self, x, y, sequence_length):\n",
    "        self.x = x\n",
    "        self.y = y #shifted close price\n",
    "        self.seq_length = sequence_length #can be thought of as the window size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #length means how many sequences\n",
    "        return self.x.shape[0] - self.seq_length\n",
    "\n",
    "    #assuming __getsize__ will be used by data loader to figure out when to stop\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #return sequences\n",
    "        #return\n",
    "\n",
    "        return Tensor(self.x.iloc[[idx]].values), Tensor(self.y.iloc[[idx]].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "dataset_train = StockDataset(X_train, y_train, 30)\n",
    "dataset_val = StockDataset(X_val, y_val, 30)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "      order          Date       Open       High        Low      Close  \\\n8423   8423  1.399421e+09  18.939178  19.004085  18.699913  18.846272   \n8424   8424  1.399507e+09  18.820993  19.018083  18.761804  18.812675   \n8425   8425  1.399594e+09  18.702297  18.757009  18.567598  18.734293   \n8426   8426  1.399853e+09  18.796681  18.994088  18.793800  18.967533   \n8427   8427  1.399939e+09  18.940975  19.022242  18.899384  18.997286   \n\n         Volume  \n8423  282864400  \n8424  230297200  \n8425  291597600  \n8426  213208800  \n8427  159737200  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order</th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8423</th>\n      <td>8423</td>\n      <td>1.399421e+09</td>\n      <td>18.939178</td>\n      <td>19.004085</td>\n      <td>18.699913</td>\n      <td>18.846272</td>\n      <td>282864400</td>\n    </tr>\n    <tr>\n      <th>8424</th>\n      <td>8424</td>\n      <td>1.399507e+09</td>\n      <td>18.820993</td>\n      <td>19.018083</td>\n      <td>18.761804</td>\n      <td>18.812675</td>\n      <td>230297200</td>\n    </tr>\n    <tr>\n      <th>8425</th>\n      <td>8425</td>\n      <td>1.399594e+09</td>\n      <td>18.702297</td>\n      <td>18.757009</td>\n      <td>18.567598</td>\n      <td>18.734293</td>\n      <td>291597600</td>\n    </tr>\n    <tr>\n      <th>8426</th>\n      <td>8426</td>\n      <td>1.399853e+09</td>\n      <td>18.796681</td>\n      <td>18.994088</td>\n      <td>18.793800</td>\n      <td>18.967533</td>\n      <td>213208800</td>\n    </tr>\n    <tr>\n      <th>8427</th>\n      <td>8427</td>\n      <td>1.399939e+09</td>\n      <td>18.940975</td>\n      <td>19.022242</td>\n      <td>18.899384</td>\n      <td>18.997286</td>\n      <td>159737200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "    order         Date      Open      High       Low     Close     Volume\n0       0  345427200.0  0.100323  0.100759  0.100323  0.100323  469033600\n1       1  345686400.0  0.095525  0.095525  0.095089  0.095089  175884800\n2       2  345772800.0  0.088546  0.088546  0.088110  0.088110  105728000\n3       3  345859200.0  0.090291  0.090727  0.090291  0.090291   86441600\n4       4  345945600.0  0.092908  0.093345  0.092908  0.092908   73449600\n5       5  346032000.0  0.098578  0.099015  0.098578  0.098578   48630400\n6       6  346291200.0  0.103376  0.103813  0.103376  0.103376   37363200\n7       7  346377600.0  0.107739  0.108175  0.107739  0.107739   46950400\n8       8  346464000.0  0.113409  0.113845  0.113409  0.113409   48003200\n9       9  346636800.0  0.123877  0.124314  0.123877  0.123877   55574400\n10     10  346896000.0  0.125622  0.126058  0.125622  0.125622   93161600\n11     11  346982400.0  0.123005  0.123005  0.122569  0.122569   68880000\n12     12  347068800.0  0.119516  0.119516  0.119079  0.119079   35750400\n13     13  347241600.0  0.120388  0.121260  0.120388  0.120388   21660800\n14     14  347500800.0  0.118207  0.118207  0.117771  0.117771   35728000\n15     15  347587200.0  0.112973  0.112973  0.112536  0.112536   45158400\n16     16  347673600.0  0.108175  0.108175  0.107739  0.107739   55686400",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order</th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>345427200.0</td>\n      <td>0.100323</td>\n      <td>0.100759</td>\n      <td>0.100323</td>\n      <td>0.100323</td>\n      <td>469033600</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>345686400.0</td>\n      <td>0.095525</td>\n      <td>0.095525</td>\n      <td>0.095089</td>\n      <td>0.095089</td>\n      <td>175884800</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>345772800.0</td>\n      <td>0.088546</td>\n      <td>0.088546</td>\n      <td>0.088110</td>\n      <td>0.088110</td>\n      <td>105728000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>345859200.0</td>\n      <td>0.090291</td>\n      <td>0.090727</td>\n      <td>0.090291</td>\n      <td>0.090291</td>\n      <td>86441600</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>345945600.0</td>\n      <td>0.092908</td>\n      <td>0.093345</td>\n      <td>0.092908</td>\n      <td>0.092908</td>\n      <td>73449600</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>346032000.0</td>\n      <td>0.098578</td>\n      <td>0.099015</td>\n      <td>0.098578</td>\n      <td>0.098578</td>\n      <td>48630400</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>346291200.0</td>\n      <td>0.103376</td>\n      <td>0.103813</td>\n      <td>0.103376</td>\n      <td>0.103376</td>\n      <td>37363200</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>346377600.0</td>\n      <td>0.107739</td>\n      <td>0.108175</td>\n      <td>0.107739</td>\n      <td>0.107739</td>\n      <td>46950400</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>346464000.0</td>\n      <td>0.113409</td>\n      <td>0.113845</td>\n      <td>0.113409</td>\n      <td>0.113409</td>\n      <td>48003200</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>346636800.0</td>\n      <td>0.123877</td>\n      <td>0.124314</td>\n      <td>0.123877</td>\n      <td>0.123877</td>\n      <td>55574400</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>346896000.0</td>\n      <td>0.125622</td>\n      <td>0.126058</td>\n      <td>0.125622</td>\n      <td>0.125622</td>\n      <td>93161600</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>346982400.0</td>\n      <td>0.123005</td>\n      <td>0.123005</td>\n      <td>0.122569</td>\n      <td>0.122569</td>\n      <td>68880000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>347068800.0</td>\n      <td>0.119516</td>\n      <td>0.119516</td>\n      <td>0.119079</td>\n      <td>0.119079</td>\n      <td>35750400</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>347241600.0</td>\n      <td>0.120388</td>\n      <td>0.121260</td>\n      <td>0.120388</td>\n      <td>0.120388</td>\n      <td>21660800</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>347500800.0</td>\n      <td>0.118207</td>\n      <td>0.118207</td>\n      <td>0.117771</td>\n      <td>0.117771</td>\n      <td>35728000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>347587200.0</td>\n      <td>0.112973</td>\n      <td>0.112973</td>\n      <td>0.112536</td>\n      <td>0.112536</td>\n      <td>45158400</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>347673600.0</td>\n      <td>0.108175</td>\n      <td>0.108175</td>\n      <td>0.107739</td>\n      <td>0.107739</td>\n      <td>55686400</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[[i for i in range(0,17)]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[3.8600e+03, 8.2728e+08, 1.9687e-01, 1.9687e-01, 1.9209e-01,\n",
      "          1.9304e-01, 1.1599e+08]],\n",
      "\n",
      "        [[2.7500e+03, 6.8869e+08, 3.7502e-01, 3.7866e-01, 3.6956e-01,\n",
      "          3.7684e-01, 1.0124e+08]],\n",
      "\n",
      "        [[6.9270e+03, 1.2119e+09, 5.7312e+00, 5.7477e+00, 5.6184e+00,\n",
      "          5.7190e+00, 7.4398e+08]],\n",
      "\n",
      "        [[3.3400e+02, 3.8742e+08, 6.1066e-02, 6.1503e-02, 6.0630e-02,\n",
      "          6.0630e-02, 4.4307e+07]],\n",
      "\n",
      "        [[4.8610e+03, 9.5247e+08, 9.3942e-01, 9.4754e-01, 9.0645e-01,\n",
      "          9.3273e-01, 2.7123e+08]],\n",
      "\n",
      "        [[1.9960e+03, 5.9452e+08, 2.6280e-01, 2.6456e-01, 2.5927e-01,\n",
      "          2.6191e-01, 2.4246e+08]],\n",
      "\n",
      "        [[2.4960e+03, 6.5690e+08, 2.1443e-01, 2.2524e-01, 2.1443e-01,\n",
      "          2.1623e-01, 1.3420e+08]],\n",
      "\n",
      "        [[1.0460e+03, 4.7598e+08, 1.0425e-01, 1.0469e-01, 1.0120e-01,\n",
      "          1.0120e-01, 2.7624e+08]],\n",
      "\n",
      "        [[2.1160e+03, 6.0964e+08, 2.8018e-01, 2.8372e-01, 2.7663e-01,\n",
      "          2.7929e-01, 1.3938e+08]],\n",
      "\n",
      "        [[7.3770e+03, 1.2683e+09, 6.8474e+00, 6.8961e+00, 6.8294e+00,\n",
      "          6.8961e+00, 4.0570e+08]],\n",
      "\n",
      "        [[3.8170e+03, 8.2192e+08, 2.5134e-01, 2.5516e-01, 2.3223e-01,\n",
      "          2.4417e-01, 6.9839e+08]],\n",
      "\n",
      "        [[2.0360e+03, 5.9979e+08, 2.8471e-01, 2.8648e-01, 2.8294e-01,\n",
      "          2.8560e-01, 1.0002e+08]],\n",
      "\n",
      "        [[3.6520e+03, 8.0127e+08, 3.3161e-01, 3.3540e-01, 3.2498e-01,\n",
      "          3.2972e-01, 2.6467e+08]],\n",
      "\n",
      "        [[2.6120e+03, 6.7167e+08, 4.4750e-01, 4.6743e-01, 4.3482e-01,\n",
      "          4.5112e-01, 1.7004e+09]]]), tensor([[0.1921],\n",
      "        [0.3623],\n",
      "        [5.7092],\n",
      "        [0.0558],\n",
      "        [0.9346],\n",
      "        [0.2663],\n",
      "        [0.2153],\n",
      "        [0.0999],\n",
      "        [0.2766],\n",
      "        [6.9297],\n",
      "        [0.2284],\n",
      "        [0.2971],\n",
      "        [0.3288],\n",
      "        [0.4656]])]\n"
     ]
    }
   ],
   "source": [
    "iterable = iter(train_dataloader)\n",
    "*_, last = iterable\n",
    "print(last)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[[1.6000e+01, 3.4767e+08, 1.0817e-01, 1.0817e-01, 1.0774e-01,\n           1.0774e-01, 5.5686e+07]],\n \n         [[1.7000e+01, 3.4776e+08, 1.0599e-01, 1.0599e-01, 1.0556e-01,\n           1.0556e-01, 3.9827e+07]],\n \n         [[1.8000e+01, 3.4785e+08, 1.1123e-01, 1.1166e-01, 1.1123e-01,\n           1.1123e-01, 2.1504e+07]],\n \n         [[1.9000e+01, 3.4811e+08, 1.1123e-01, 1.1123e-01, 1.1036e-01,\n           1.1036e-01, 2.3699e+07]],\n \n         [[2.0000e+01, 3.4819e+08, 1.0687e-01, 1.0687e-01, 1.0643e-01,\n           1.0643e-01, 2.3050e+07]],\n \n         [[2.1000e+01, 3.4828e+08, 1.0687e-01, 1.0730e-01, 1.0687e-01,\n           1.0687e-01, 1.4291e+07]],\n \n         [[2.2000e+01, 3.4836e+08, 1.0905e-01, 1.0992e-01, 1.0905e-01,\n           1.0905e-01, 1.4067e+07]],\n \n         [[2.3000e+01, 3.4845e+08, 1.0861e-01, 1.0861e-01, 1.0817e-01,\n           1.0817e-01, 1.3395e+07]],\n \n         [[2.4000e+01, 3.4871e+08, 1.1472e-01, 1.1515e-01, 1.1472e-01,\n           1.1472e-01, 4.1574e+07]],\n \n         [[2.5000e+01, 3.4880e+08, 1.1166e-01, 1.1166e-01, 1.1123e-01,\n           1.1123e-01, 3.0083e+07]],\n \n         [[2.6000e+01, 3.4888e+08, 1.1341e-01, 1.1428e-01, 1.1341e-01,\n           1.1341e-01, 1.5904e+07]],\n \n         [[2.7000e+01, 3.4897e+08, 1.1472e-01, 1.1559e-01, 1.1472e-01,\n           1.1472e-01, 3.5549e+07]],\n \n         [[2.8000e+01, 3.4906e+08, 1.1472e-01, 1.1515e-01, 1.1428e-01,\n           1.1428e-01, 1.1222e+07]],\n \n         [[2.9000e+01, 3.4932e+08, 1.1297e-01, 1.1297e-01, 1.1254e-01,\n           1.1254e-01, 2.4640e+07]],\n \n         [[3.0000e+01, 3.4940e+08, 1.1254e-01, 1.1254e-01, 1.1166e-01,\n           1.1166e-01, 2.3699e+07]],\n \n         [[3.1000e+01, 3.4949e+08, 1.0861e-01, 1.0861e-01, 1.0817e-01,\n           1.0817e-01, 2.8157e+07]]]),\n tensor([[0.1056],\n         [0.1112],\n         [0.1104],\n         [0.1064],\n         [0.1069],\n         [0.1090],\n         [0.1082],\n         [0.1147],\n         [0.1112],\n         [0.1134],\n         [0.1147],\n         [0.1143],\n         [0.1125],\n         [0.1117],\n         [0.1082],\n         [0.1042]])]"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second = next(iterable)\n",
    "second"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   date      open        close       high       low    volume       target  \\\n0     1  0.375879  1000.192535  18.738922  2.274733  2.790435  1000.404939   \n1     2  0.434617  1000.404939  18.355642  2.369738  2.848000  1000.732609   \n2     3  1.167940  1000.732609  18.305610  2.467036  2.082916  1000.470478   \n3     4  0.675451  1000.470478  18.195614  2.061493  2.797018  1000.834811   \n4     5  1.002993  1000.834811  18.729299  2.860682  2.823988  1000.111026   \n\n   constant  \n0       1.0  \n1       1.0  \n2       1.0  \n3       1.0  \n4       1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>close</th>\n      <th>high</th>\n      <th>low</th>\n      <th>volume</th>\n      <th>target</th>\n      <th>constant</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.375879</td>\n      <td>1000.192535</td>\n      <td>18.738922</td>\n      <td>2.274733</td>\n      <td>2.790435</td>\n      <td>1000.404939</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.434617</td>\n      <td>1000.404939</td>\n      <td>18.355642</td>\n      <td>2.369738</td>\n      <td>2.848000</td>\n      <td>1000.732609</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.167940</td>\n      <td>1000.732609</td>\n      <td>18.305610</td>\n      <td>2.467036</td>\n      <td>2.082916</td>\n      <td>1000.470478</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.675451</td>\n      <td>1000.470478</td>\n      <td>18.195614</td>\n      <td>2.061493</td>\n      <td>2.797018</td>\n      <td>1000.834811</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1.002993</td>\n      <td>1000.834811</td>\n      <td>18.729299</td>\n      <td>2.860682</td>\n      <td>2.823988</td>\n      <td>1000.111026</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_timeseries = pd.DataFrame(\n",
    "    dict(\n",
    "    date=[x for x in range(1,2001)],\n",
    "    open=np.random.rand(2000) + 0.2,\n",
    "    close=np.random.rand(2000) + 1000,\n",
    "        high=np.random.rand(2000) + 18,\n",
    "        low = np.random.rand(2000) + 2,\n",
    "        volume=np.random.rand(2000) + 2\n",
    "    ))\n",
    "random_timeseries[\"target\"] = random_timeseries.close.shift(-1)\n",
    "random_timeseries.fillna(3, inplace=True)\n",
    "random_timeseries[\"constant\"] = np.ones(2000)\n",
    "random_timeseries.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#what features constitute a single timeseries sample\n",
    "group_ids = ['constant']\n",
    "target='target'\n",
    "time_idx = 'date'\n",
    "\n",
    "max_prediction_length=1 #one day\n",
    "#list of continous variables that change over time and are not known in the future\n",
    "time_varying_unknown_reals = ['open', 'close', 'high', 'low', 'volume', 'target']\n",
    "#normalizer, add later\n",
    "#target_normalizer= TorchNormalizer()\n",
    "max_encoder_length = 5\n",
    "min_encoder_length = 0\n",
    "\n",
    "#add no\n",
    "pytorch_random_dataset = TimeSeriesDataSet(random_timeseries,\n",
    "                                           group_ids=group_ids,\n",
    "                                           time_idx=time_idx,\n",
    "                                           target=target,\n",
    "                                           min_prediction_length=1,\n",
    "                                           max_prediction_length=max_prediction_length,\n",
    "                                           max_encoder_length=max_encoder_length,\n",
    "                                           min_encoder_length=min_encoder_length,\n",
    "                                           allow_missing_timesteps=True,\n",
    "                                           time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "                                           #target_normalizer=target_normalizer\n",
    "                                           )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#check out dataloader which we will use to feed data to the model's forward method\n",
    "data_loader = pytorch_random_dataset.to_dataloader(batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagema/anaconda3/envs/csc492_v2/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(data_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = {'encoder_cat': tensor([], size=(16, 5, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.0000e+00, -1.1349e+00, -1.0375e+00,  8.5500e-01, -7.7623e-01,\n",
      "           1.0069e+00,  1.8451e-02],\n",
      "         [ 1.0000e+00, -9.3160e-01, -3.0177e-01, -4.6928e-01, -4.4722e-01,\n",
      "           1.2050e+00,  3.3144e-02],\n",
      "         [ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n",
      "          -1.4283e+00,  2.1390e-02],\n",
      "         [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n",
      "           1.0295e+00,  3.7727e-02],\n",
      "         [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n",
      "           1.1223e+00,  5.2715e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -9.3160e-01, -3.0177e-01, -4.6928e-01, -4.4722e-01,\n",
      "           1.2050e+00,  3.3144e-02],\n",
      "         [ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n",
      "          -1.4283e+00,  2.1390e-02],\n",
      "         [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n",
      "           1.0295e+00,  3.7727e-02],\n",
      "         [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n",
      "           1.1223e+00,  5.2715e-03],\n",
      "         [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n",
      "          -1.4283e+00,  2.1390e-02],\n",
      "         [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n",
      "           1.0295e+00,  3.7727e-02],\n",
      "         [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n",
      "           1.1223e+00,  5.2715e-03],\n",
      "         [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02],\n",
      "         [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n",
      "           1.0295e+00,  3.7727e-02],\n",
      "         [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n",
      "           1.1223e+00,  5.2715e-03],\n",
      "         [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02],\n",
      "         [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02],\n",
      "         [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n",
      "           1.1223e+00,  5.2715e-03],\n",
      "         [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02],\n",
      "         [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02],\n",
      "         [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03],\n",
      "         [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02],\n",
      "         [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02],\n",
      "         [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03],\n",
      "         [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02],\n",
      "         [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02],\n",
      "         [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03],\n",
      "         [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02],\n",
      "         [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03],\n",
      "         [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03],\n",
      "         [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02],\n",
      "         [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03],\n",
      "         [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02],\n",
      "         [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02],\n",
      "         [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03],\n",
      "         [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02],\n",
      "         [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03],\n",
      "         [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03],\n",
      "         [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02],\n",
      "         [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03],\n",
      "         [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02],\n",
      "         [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02],\n",
      "         [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03],\n",
      "         [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02],\n",
      "         [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02],\n",
      "         [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03],\n",
      "         [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02],\n",
      "         [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02],\n",
      "         [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02],\n",
      "         [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02],\n",
      "         [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02],\n",
      "         [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02],\n",
      "         [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02],\n",
      "         [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n",
      "          -1.0624e-01,  2.7577e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02],\n",
      "         [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02],\n",
      "         [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02],\n",
      "         [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n",
      "          -1.0624e-01,  2.7577e-02],\n",
      "         [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n",
      "          -3.8290e-01,  4.1103e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02],\n",
      "         [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02],\n",
      "         [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n",
      "          -1.0624e-01,  2.7577e-02],\n",
      "         [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n",
      "          -3.8290e-01,  4.1103e-02],\n",
      "         [ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n",
      "          -9.3962e-01,  2.0780e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02],\n",
      "         [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n",
      "          -1.0624e-01,  2.7577e-02],\n",
      "         [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n",
      "          -3.8290e-01,  4.1103e-02],\n",
      "         [ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n",
      "          -9.3962e-01,  2.0780e-02],\n",
      "         [ 1.0000e+00, -1.0906e+00, -1.2183e-01,  1.0359e-01,  1.1466e+00,\n",
      "          -1.5504e+00,  1.9083e-02]]]), 'encoder_target': tensor([[1000.4050, 1000.7326, 1000.4705, 1000.8348, 1000.1110],\n",
      "        [1000.7326, 1000.4705, 1000.8348, 1000.1110, 1000.9887],\n",
      "        [1000.4705, 1000.8348, 1000.1110, 1000.9887, 1000.8973],\n",
      "        [1000.8348, 1000.1110, 1000.9887, 1000.8973, 1000.0219],\n",
      "        [1000.1110, 1000.9887, 1000.8973, 1000.0219, 1000.2546],\n",
      "        [1000.9887, 1000.8973, 1000.0219, 1000.2546, 1000.2086],\n",
      "        [1000.8973, 1000.0219, 1000.2546, 1000.2086, 1000.7730],\n",
      "        [1000.0219, 1000.2546, 1000.2086, 1000.7730, 1000.0804],\n",
      "        [1000.2546, 1000.2086, 1000.7730, 1000.0804, 1000.8469],\n",
      "        [1000.2086, 1000.7730, 1000.0804, 1000.8469, 1000.6785],\n",
      "        [1000.7730, 1000.0804, 1000.8469, 1000.6785, 1000.3835],\n",
      "        [1000.0804, 1000.8469, 1000.6785, 1000.3835, 1000.7620],\n",
      "        [1000.8469, 1000.6785, 1000.3835, 1000.7620, 1000.6085],\n",
      "        [1000.6785, 1000.3835, 1000.7620, 1000.6085, 1000.9101],\n",
      "        [1000.3835, 1000.7620, 1000.6085, 1000.9101, 1000.4569],\n",
      "        [1000.7620, 1000.6085, 1000.9101, 1000.4569, 1000.4191]]), 'encoder_lengths': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]), 'decoder_cat': tensor([], size=(16, 1, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n",
      "           5.0060e-01,  4.4627e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n",
      "          -1.0773e+00,  4.0529e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n",
      "           7.7652e-03,  1.2738e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n",
      "           3.7648e-01,  1.1710e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n",
      "          -8.9725e-01,  9.6437e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n",
      "           8.3214e-01,  3.4957e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n",
      "          -3.8115e-01,  3.8987e-03]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n",
      "          -1.0431e+00,  3.8268e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n",
      "           5.0583e-01,  3.0716e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n",
      "          -1.5223e+00,  1.7489e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n",
      "           7.7666e-01,  3.4460e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n",
      "          -1.0624e-01,  2.7577e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n",
      "          -3.8290e-01,  4.1103e-02]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n",
      "          -9.3962e-01,  2.0780e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -1.0906e+00, -1.2183e-01,  1.0359e-01,  1.1466e+00,\n",
      "          -1.5504e+00,  1.9083e-02]],\n",
      "\n",
      "        [[ 1.0000e+00, -4.8576e-01, -2.5291e-01,  1.6335e+00, -6.7710e-01,\n",
      "           3.3863e-02,  3.6437e-02]]]), 'decoder_target': tensor([[1000.9887],\n",
      "        [1000.8973],\n",
      "        [1000.0219],\n",
      "        [1000.2546],\n",
      "        [1000.2086],\n",
      "        [1000.7730],\n",
      "        [1000.0804],\n",
      "        [1000.8469],\n",
      "        [1000.6785],\n",
      "        [1000.3835],\n",
      "        [1000.7620],\n",
      "        [1000.6085],\n",
      "        [1000.9101],\n",
      "        [1000.4569],\n",
      "        [1000.4191],\n",
      "        [1000.8060]]), 'decoder_lengths': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'decoder_time_idx': tensor([[ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [11],\n",
      "        [12],\n",
      "        [13],\n",
      "        [14],\n",
      "        [15],\n",
      "        [16],\n",
      "        [17],\n",
      "        [18],\n",
      "        [19],\n",
      "        [20],\n",
      "        [21]]), 'groups': tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]), 'target_scale': tensor([[999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009],\n",
      "        [999.9935,  22.3009]])}\n",
      "\n",
      "y = (tensor([[1000.9887],\n",
      "        [1000.8973],\n",
      "        [1000.0219],\n",
      "        [1000.2546],\n",
      "        [1000.2086],\n",
      "        [1000.7730],\n",
      "        [1000.0804],\n",
      "        [1000.8469],\n",
      "        [1000.6785],\n",
      "        [1000.3835],\n",
      "        [1000.7620],\n",
      "        [1000.6085],\n",
      "        [1000.9101],\n",
      "        [1000.4569],\n",
      "        [1000.4191],\n",
      "        [1000.8060]]), None)\n",
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([16, 5, 0])\n",
      "\tencoder_cont = torch.Size([16, 5, 7])\n",
      "\tencoder_target = torch.Size([16, 5])\n",
      "\tencoder_lengths = torch.Size([16])\n",
      "\tdecoder_cat = torch.Size([16, 1, 0])\n",
      "\tdecoder_cont = torch.Size([16, 1, 7])\n",
      "\tdecoder_target = torch.Size([16, 1])\n",
      "\tdecoder_lengths = torch.Size([16])\n",
      "\tdecoder_time_idx = torch.Size([16, 1])\n",
      "\tgroups = torch.Size([16, 1])\n",
      "\ttarget_scale = torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Transformer Architecture</h3>\n",
    "\n",
    "Note that the custom encoder contains:\n",
    "- a self attention layer\n",
    "- a feed forward layer (normal stuffnot ethat for the attention layer, the multihead attention needs the number of diemnsions to be divisible by the number of multi attention heads.\n",
    "\n",
    "<h3>Model 1 (used for language)</h3> (theirs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
    "from code_repo.transformer import PositionalEncoding\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Pytorch Adapter</h3>\n",
    "<p>Currently this contains a premade transformer model from the torch fellows</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#note that this has to extract 'encoder_cont' from the input, then pass it to the saved transformer model\n",
    "\n",
    "class StockTransformerModel(BaseModel):\n",
    "\n",
    "    def __init__(self, d_model = 5, nhead = 5,\n",
    "                 num_encoder_layers = 6,\n",
    "                 num_decoder_layers = 6,\n",
    "                 dim_feedforward = 100,\n",
    "                 dropout = 0.1,\n",
    "                 activation = \"relu\"):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(d_model, num_encoder_layers,\\\n",
    "                                  num_decoder_layers, dim_feedforward,\n",
    "                                  dropout)\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.network = nn.Transformer(d_model = self.d_model,\n",
    "                                      nhead = self.nhead,\n",
    "                                      num_encoder_layers = self.num_encoder_layers,\n",
    "                                      dim_feedforward= self.dim_feedforward,\n",
    "                                      dropout= self.dropout,\n",
    "                                      batch_first=True\n",
    "                                      #activation= self.activation\n",
    "                                      )\n",
    "\n",
    "        #how many features are we passing through\n",
    "\n",
    "        #custom_encoder = some class\n",
    "        #custom_decoder = some class\n",
    "\n",
    "        #feed forward already implemented\n",
    "    def forward(self, x):\n",
    "        #need to extract data from whats returned in batch\n",
    "        new_x = x[\"encoder_cont\"].squeeze(-1)\n",
    "        new_y = x['decoder_cont'].squeeze(-1)\n",
    "\n",
    "        pred = self.network(new_x, new_y)\n",
    "\n",
    "\n",
    "        prediction = self.transform_output(pred, target_scale=x[\"target_scale\"])\n",
    "        return self.to_netowrk_output(prediction=pred)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m StockTransformerModel()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/csc492_v2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36mStockTransformerModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     40\u001B[0m new_x \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder_cont\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     41\u001B[0m new_y \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdecoder_cont\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform_output(pred, target_scale\u001B[38;5;241m=\u001B[39mx[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_netowrk_output(prediction\u001B[38;5;241m=\u001B[39mpred)\n",
      "File \u001B[0;32m~/anaconda3/envs/csc492_v2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/csc492_v2/lib/python3.9/site-packages/torch/nn/modules/transformer.py:139\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe batch number of src and tgt must be equal\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model \u001B[38;5;129;01mor\u001B[39;00m tgt\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_model:\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe feature number of src and tgt must be equal to d_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    141\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(src, mask\u001B[38;5;241m=\u001B[39msrc_mask, src_key_padding_mask\u001B[38;5;241m=\u001B[39msrc_key_padding_mask)\n\u001B[1;32m    142\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(tgt, memory, tgt_mask\u001B[38;5;241m=\u001B[39mtgt_mask, memory_mask\u001B[38;5;241m=\u001B[39mmemory_mask,\n\u001B[1;32m    143\u001B[0m                       tgt_key_padding_mask\u001B[38;5;241m=\u001B[39mtgt_key_padding_mask,\n\u001B[1;32m    144\u001B[0m                       memory_key_padding_mask\u001B[38;5;241m=\u001B[39mmemory_key_padding_mask)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "model = StockTransformerModel()\n",
    "model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'encoder_cat': tensor([], size=(16, 5, 0), dtype=torch.int64),\n 'encoder_cont': tensor([[[ 1.0000e+00, -1.1349e+00, -1.0375e+00,  8.5500e-01, -7.7623e-01,\n            1.0069e+00,  1.8451e-02],\n          [ 1.0000e+00, -9.3160e-01, -3.0177e-01, -4.6928e-01, -4.4722e-01,\n            1.2050e+00,  3.3144e-02],\n          [ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n           -1.4283e+00,  2.1390e-02],\n          [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n            1.0295e+00,  3.7727e-02],\n          [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n            1.1223e+00,  5.2715e-03]],\n \n         [[ 1.0000e+00, -9.3160e-01, -3.0177e-01, -4.6928e-01, -4.4722e-01,\n            1.2050e+00,  3.3144e-02],\n          [ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n           -1.4283e+00,  2.1390e-02],\n          [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n            1.0295e+00,  3.7727e-02],\n          [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n            1.1223e+00,  5.2715e-03],\n          [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02]],\n \n         [[ 1.0000e+00,  1.6065e+00,  8.3318e-01, -6.4214e-01, -1.1026e-01,\n           -1.4283e+00,  2.1390e-02],\n          [ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n            1.0295e+00,  3.7727e-02],\n          [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n            1.1223e+00,  5.2715e-03],\n          [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02],\n          [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02]],\n \n         [[ 1.0000e+00, -9.8043e-02, -7.4761e-02, -1.0222e+00, -1.5147e+00,\n            1.0295e+00,  3.7727e-02],\n          [ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n            1.1223e+00,  5.2715e-03],\n          [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02],\n          [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02],\n          [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03]],\n \n         [[ 1.0000e+00,  1.0356e+00,  1.1872e+00,  8.2175e-01,  1.2530e+00,\n            1.1223e+00,  5.2715e-03],\n          [ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02],\n          [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02],\n          [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03],\n          [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02]],\n \n         [[ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02],\n          [ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02],\n          [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03],\n          [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02],\n          [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03]],\n \n         [[ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02],\n          [ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03],\n          [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02],\n          [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03],\n          [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02]],\n \n         [[ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03],\n          [ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02],\n          [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03],\n          [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02],\n          [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03]],\n \n         [[ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02],\n          [ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03],\n          [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02],\n          [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03],\n          [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02]],\n \n         [[ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03],\n          [ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02],\n          [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03],\n          [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02],\n          [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02]],\n \n         [[ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02],\n          [ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03],\n          [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02],\n          [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02],\n          [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02]],\n \n         [[ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03],\n          [ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02],\n          [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02],\n          [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02],\n          [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02]],\n \n         [[ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02],\n          [ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02],\n          [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02],\n          [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02],\n          [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n           -1.0624e-01,  2.7577e-02]],\n \n         [[ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02],\n          [ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02],\n          [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02],\n          [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n           -1.0624e-01,  2.7577e-02],\n          [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n           -3.8290e-01,  4.1103e-02]],\n \n         [[ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02],\n          [ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02],\n          [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n           -1.0624e-01,  2.7577e-02],\n          [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n           -3.8290e-01,  4.1103e-02],\n          [ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n           -9.3962e-01,  2.0780e-02]],\n \n         [[ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02],\n          [ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n           -1.0624e-01,  2.7577e-02],\n          [ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n           -3.8290e-01,  4.1103e-02],\n          [ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n           -9.3962e-01,  2.0780e-02],\n          [ 1.0000e+00, -1.0906e+00, -1.2183e-01,  1.0359e-01,  1.1466e+00,\n           -1.5504e+00,  1.9083e-02]]]),\n 'encoder_target': tensor([[1000.4050, 1000.7326, 1000.4705, 1000.8348, 1000.1110],\n         [1000.7326, 1000.4705, 1000.8348, 1000.1110, 1000.9887],\n         [1000.4705, 1000.8348, 1000.1110, 1000.9887, 1000.8973],\n         [1000.8348, 1000.1110, 1000.9887, 1000.8973, 1000.0219],\n         [1000.1110, 1000.9887, 1000.8973, 1000.0219, 1000.2546],\n         [1000.9887, 1000.8973, 1000.0219, 1000.2546, 1000.2086],\n         [1000.8973, 1000.0219, 1000.2546, 1000.2086, 1000.7730],\n         [1000.0219, 1000.2546, 1000.2086, 1000.7730, 1000.0804],\n         [1000.2546, 1000.2086, 1000.7730, 1000.0804, 1000.8469],\n         [1000.2086, 1000.7730, 1000.0804, 1000.8469, 1000.6785],\n         [1000.7730, 1000.0804, 1000.8469, 1000.6785, 1000.3835],\n         [1000.0804, 1000.8469, 1000.6785, 1000.3835, 1000.7620],\n         [1000.8469, 1000.6785, 1000.3835, 1000.7620, 1000.6085],\n         [1000.6785, 1000.3835, 1000.7620, 1000.6085, 1000.9101],\n         [1000.3835, 1000.7620, 1000.6085, 1000.9101, 1000.4569],\n         [1000.7620, 1000.6085, 1000.9101, 1000.4569, 1000.4191]]),\n 'encoder_lengths': tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]),\n 'decoder_cat': tensor([], size=(16, 1, 0), dtype=torch.int64),\n 'decoder_cont': tensor([[[ 1.0000e+00,  7.1542e-01, -1.3198e+00,  7.6118e-01,  2.6301e-01,\n            5.0060e-01,  4.4627e-02]],\n \n         [[ 1.0000e+00,  1.5232e+00,  1.7202e+00,  6.1780e-01, -1.6836e+00,\n           -1.0773e+00,  4.0529e-02]],\n \n         [[ 1.0000e+00,  2.7391e-02,  1.4036e+00, -6.5653e-01,  9.5132e-01,\n            7.7652e-03,  1.2738e-03]],\n \n         [[ 1.0000e+00, -5.6774e-01, -1.6286e+00, -3.9792e-01,  6.3433e-01,\n            3.7648e-01,  1.1710e-02]],\n \n         [[ 1.0000e+00, -1.2889e+00, -8.2249e-01,  1.4839e+00, -1.6226e+00,\n           -8.9725e-01,  9.6437e-03]],\n \n         [[ 1.0000e+00, -1.1976e-01, -9.8207e-01,  9.4391e-01,  3.3746e-01,\n            8.3214e-01,  3.4957e-02]],\n \n         [[ 1.0000e+00, -7.2218e-01,  9.7318e-01, -1.5302e+00,  1.0029e+00,\n           -3.8115e-01,  3.8987e-03]],\n \n         [[ 1.0000e+00,  1.6561e+00, -1.4258e+00, -1.5687e+00, -1.1726e+00,\n           -1.0431e+00,  3.8268e-02]],\n \n         [[ 1.0000e+00, -1.0992e+00,  1.2289e+00,  4.6518e-01,  4.2372e-01,\n            5.0583e-01,  3.0716e-02]],\n \n         [[ 1.0000e+00,  1.6819e+00,  6.4565e-01,  8.1379e-01, -1.2654e+00,\n           -1.5223e+00,  1.7489e-02]],\n \n         [[ 1.0000e+00, -1.5742e+00, -3.7606e-01,  1.1521e+00, -1.4046e+00,\n            7.7666e-01,  3.4460e-02]],\n \n         [[ 1.0000e+00,  2.5272e-01,  9.3479e-01, -5.7661e-01,  7.8147e-01,\n           -1.0624e-01,  2.7577e-02]],\n \n         [[ 1.0000e+00, -3.8295e-01,  4.0318e-01, -1.4592e+00,  6.0873e-01,\n           -3.8290e-01,  4.1103e-02]],\n \n         [[ 1.0000e+00,  1.3774e+00,  1.4480e+00, -1.1021e+00,  1.5650e-01,\n           -9.3962e-01,  2.0780e-02]],\n \n         [[ 1.0000e+00, -1.0906e+00, -1.2183e-01,  1.0359e-01,  1.1466e+00,\n           -1.5504e+00,  1.9083e-02]],\n \n         [[ 1.0000e+00, -4.8576e-01, -2.5291e-01,  1.6335e+00, -6.7710e-01,\n            3.3863e-02,  3.6437e-02]]]),\n 'decoder_target': tensor([[1000.9887],\n         [1000.8973],\n         [1000.0219],\n         [1000.2546],\n         [1000.2086],\n         [1000.7730],\n         [1000.0804],\n         [1000.8469],\n         [1000.6785],\n         [1000.3835],\n         [1000.7620],\n         [1000.6085],\n         [1000.9101],\n         [1000.4569],\n         [1000.4191],\n         [1000.8060]]),\n 'decoder_lengths': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'decoder_time_idx': tensor([[ 6],\n         [ 7],\n         [ 8],\n         [ 9],\n         [10],\n         [11],\n         [12],\n         [13],\n         [14],\n         [15],\n         [16],\n         [17],\n         [18],\n         [19],\n         [20],\n         [21]]),\n 'groups': tensor([[0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0]]),\n 'target_scale': tensor([[999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009],\n         [999.9935,  22.3009]])}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 5, 7, 16, 1])"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['encoder_cont'].size() + y[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [107]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43myx\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoder_cont\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msize()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'yx' is not defined"
     ]
    }
   ],
   "source": [
    "x['encoder_cont'].size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train model\n",
    "def train(model, data, batch_size=32,  learning_rate=0.1, momentum=0.9, total_epochs=10, weight_decay=0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer= optim.Adam(model.parameters(), learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    iterations = []\n",
    "    training_accuract = []\n",
    "    validation_accuract = []\n",
    "\n",
    "    num_iterations = 0\n",
    "    for epoch in range(0, total_epochs):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.parameters at 0x7f96c84cfc80>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StockTransformer()\n",
    "model.parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "src = torch.rand((10, 32, 6))\n",
    "tgt = torch.rand((20,32,6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "out = model(src, tgt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.rand((1,2,3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.8609e+00,  3.5001e-01,  5.2951e-01,  ..., -5.6568e-01,\n          -3.1149e-01, -8.8702e-01],\n         [ 1.3641e+00,  3.6736e-02,  6.4878e-01,  ...,  4.2819e-01,\n          -7.7870e-01,  5.3546e-01],\n         [ 1.5085e+00,  6.7698e-01,  5.3151e-01,  ...,  4.6038e-01,\n           2.2322e-01, -1.2730e+00],\n         ...,\n         [ 1.8146e+00,  7.0981e-01,  4.2588e-01,  ...,  9.0969e-01,\n           4.8604e-02,  1.7219e-01],\n         [ 1.8847e+00,  6.4794e-02,  1.6133e-01,  ...,  1.7033e-01,\n          -6.5030e-01,  1.1771e+00],\n         [ 1.1817e+00,  6.8779e-01,  5.2459e-01,  ..., -3.5132e-01,\n          -3.7258e-01, -5.6009e-02]],\n\n        [[ 1.0215e+00,  7.4425e-01,  1.1591e+00,  ...,  2.7391e-01,\n          -2.9277e-01, -1.4802e-01],\n         [ 1.9489e+00,  1.6937e-01,  2.9322e-01,  ...,  6.9300e-01,\n          -1.0268e+00,  3.5865e-01],\n         [ 1.4858e+00,  6.8375e-01,  1.0703e+00,  ...,  7.4776e-01,\n           2.1364e-01,  4.6723e-02],\n         ...,\n         [ 1.9812e+00,  7.7249e-01,  1.1114e+00,  ...,  5.7714e-01,\n          -4.7355e-01,  1.9327e-01],\n         [ 2.0568e+00,  8.3835e-01,  7.9678e-01,  ...,  2.3893e-02,\n          -5.6904e-01,  5.2341e-01],\n         [ 1.2154e+00,  1.3414e+00,  1.0384e+00,  ..., -4.8584e-01,\n           1.6820e-01,  8.4261e-01]],\n\n        [[ 1.4005e+00, -3.9579e-03,  2.6295e-01,  ...,  2.6633e-01,\n          -8.8358e-01,  2.9011e-01],\n         [ 1.5764e+00,  2.6680e-01,  7.0202e-01,  ...,  7.1652e-01,\n          -7.5740e-01,  1.5319e-01],\n         [ 9.2704e-01,  3.3642e-01,  7.9277e-01,  ...,  5.9988e-01,\n          -3.7554e-01, -2.7791e-01],\n         ...,\n         [ 1.3096e+00,  7.1499e-01,  1.1997e+00,  ..., -3.4324e-01,\n          -2.3558e-01, -1.1478e-01],\n         [ 1.6793e+00,  5.9849e-01,  9.4259e-01,  ...,  3.0393e-02,\n          -7.1690e-01, -2.8095e-01],\n         [ 1.8460e+00,  8.6415e-01,  1.0320e+00,  ..., -2.6610e-01,\n          -4.8128e-01,  1.4793e-01]],\n\n        ...,\n\n        [[ 1.6270e+00,  5.1395e-01,  1.5575e+00,  ...,  4.4120e-01,\n           8.6302e-01,  2.7452e-01],\n         [ 1.2132e+00, -5.2067e-02,  6.1256e-01,  ...,  2.4633e-01,\n          -4.6300e-01,  5.4387e-02],\n         [ 1.5218e+00,  2.3298e-01,  1.1459e+00,  ..., -6.5787e-01,\n          -2.7270e-01, -6.9753e-01],\n         ...,\n         [ 2.1243e+00,  1.0449e+00,  1.1757e-01,  ...,  5.7469e-01,\n          -6.2031e-01,  6.1834e-01],\n         [ 1.9299e+00,  7.7909e-01,  1.6217e-01,  ...,  4.4374e-01,\n          -5.3825e-01,  3.7104e-01],\n         [ 1.7415e+00,  1.0361e+00,  5.6567e-01,  ..., -1.5856e-01,\n          -2.6418e-01,  4.7155e-01]],\n\n        [[ 8.2413e-01, -1.4846e-01,  2.9891e-01,  ...,  2.3019e-01,\n           2.7850e-04, -1.0360e-01],\n         [ 1.3581e+00,  2.4826e-01,  2.9452e-01,  ...,  8.4746e-01,\n          -5.0659e-01,  1.1244e+00],\n         [ 1.1608e+00,  9.2114e-01,  9.6576e-01,  ..., -2.1513e-01,\n          -1.1932e+00, -2.6927e-01],\n         ...,\n         [ 7.6091e-01,  4.5270e-01,  4.6338e-01,  ...,  3.5564e-01,\n          -1.0805e+00, -1.1355e+00],\n         [ 1.2467e+00,  1.2111e+00,  1.0467e+00,  ...,  1.3540e-01,\n          -2.1274e-01, -7.5047e-01],\n         [ 6.0060e-01,  9.8755e-01,  7.9962e-01,  ..., -1.2321e-02,\n           2.5219e-01,  1.6050e-01]],\n\n        [[ 1.7934e+00,  7.5683e-01,  4.1987e-01,  ...,  3.7590e-02,\n          -1.4422e-01,  1.1782e-01],\n         [ 2.5753e+00,  3.7607e-01,  8.3770e-01,  ...,  4.5608e-01,\n          -5.3591e-01, -2.3224e-01],\n         [ 1.8376e+00,  3.0043e-01,  1.4269e+00,  ...,  5.1100e-01,\n          -4.1165e-01, -1.5803e-01],\n         ...,\n         [ 1.7462e+00,  7.0657e-01,  6.4432e-01,  ...,  1.2101e+00,\n          -1.7989e-01, -5.2918e-01],\n         [ 2.0691e+00,  5.2253e-01,  1.2977e+00,  ..., -1.9132e-01,\n          -3.9879e-01, -2.6848e-02],\n         [ 1.0723e+00,  7.7057e-01,  6.4880e-01,  ..., -4.1994e-01,\n          -3.8130e-01,  2.6043e-01]]], grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "\n",
    "timesteps = 1000\n",
    "\n",
    "data = generate_ar_data(seasonality=10.0, timesteps=timesteps, n_series=100, seed=42)\n",
    "data[\"static\"] = 2\n",
    "data[\"date\"] = pd.Timestamp(\"2020-01-01\") + pd.to_timedelta(data.time_idx, \"D\")\n",
    "\n",
    "data.series = data.series.astype(str).astype(\"category\")\n",
    "\n",
    "max_encoder_length = 30\n",
    "max_prediction_length = 15\n",
    "\n",
    "cutoff = timesteps * 0.70\n",
    "train_data = data[data[\"time_idx\"] <= cutoff]\n",
    "test_data = data[data[\"time_idx\"] > cutoff]\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    train_data,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "#      categorical_encoders={\"series\": NaNLabelEncoder().fit(train_data.series)},\n",
    "    group_ids=[\"series\"],\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    # allow_missing_timesteps=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Linear(5, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model * 2, 1)\n",
    "\n",
    "        self.init_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sloan stuff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Linear(5, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model * 2, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = torch.concat([torch.max(output, dim=1)[0], torch.mean(output, dim=1)], dim=1)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "       value  group  time_idx\n0   0.104145      0         0\n1  -0.408156      0         1\n2  -0.027650      0         2\n3   0.468365      0         3\n4  -0.188278      0         4\n5  -0.173244      0         5\n6  -0.285444      0         6\n7  -0.124239      0         7\n8  -0.262205      0         8\n9   0.174174      0         9\n10 -0.324031      1         0\n11  0.304083      1         1\n12 -0.441632      1         2\n13  0.200891      1         3\n14 -0.128754      1         4\n15 -0.082647      1         5\n16 -0.429988      1         6\n17 -0.499547      1         7\n18  0.387851      1         8\n19 -0.204230      1         9\n20 -0.057724      2         0\n21  0.128085      2         1\n22  0.053239      2         2\n23  0.328398      2         3\n24 -0.325298      2         4\n25 -0.431351      2         5\n26  0.135290      2         6\n27 -0.137431      2         7\n28  0.182330      2         8\n29  0.394359      2         9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>group</th>\n      <th>time_idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.104145</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.408156</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.027650</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.468365</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.188278</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.173244</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.285444</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.124239</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.262205</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.174174</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.324031</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.304083</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-0.441632</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.200891</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.128754</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.082647</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.429988</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.499547</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.387851</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.204230</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.057724</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.128085</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.053239</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.328398</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.325298</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.431351</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.135290</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>-0.137431</td>\n      <td>2</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.182330</td>\n      <td>2</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.394359</td>\n      <td>2</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.DataFrame(\n",
    "    dict(\n",
    "        value=np.random.rand(30) - 0.5,\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "    )\n",
    ")\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# create the dataset from the pandas dataframe\n",
    "dataset = TimeSeriesDataSet(\n",
    "    test_data,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"value\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = {'encoder_cat': tensor([], size=(4, 5, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 0.1013],\n",
      "         [ 1.8939],\n",
      "         [-0.4792],\n",
      "         [-0.4248],\n",
      "         [-0.8303]],\n",
      "\n",
      "        [[-1.3947],\n",
      "         [ 0.9272],\n",
      "         [-0.2640],\n",
      "         [-0.0974],\n",
      "         [-1.3527]],\n",
      "\n",
      "        [[-1.2738],\n",
      "         [ 0.1013],\n",
      "         [ 1.8939],\n",
      "         [-0.4792],\n",
      "         [-0.4248]],\n",
      "\n",
      "        [[ 0.3936],\n",
      "         [ 1.3880],\n",
      "         [-0.9743],\n",
      "         [-1.3576],\n",
      "         [ 0.6902]]]), 'encoder_target': tensor([[-0.0276,  0.4684, -0.1883, -0.1732, -0.2854],\n",
      "        [-0.4416,  0.2009, -0.1288, -0.0826, -0.4300],\n",
      "        [-0.4082, -0.0276,  0.4684, -0.1883, -0.1732],\n",
      "        [ 0.0532,  0.3284, -0.3253, -0.4314,  0.1353]]), 'encoder_lengths': tensor([5, 5, 5, 5]), 'decoder_cat': tensor([], size=(4, 2, 0), dtype=torch.int64), 'decoder_cont': tensor([[[-0.2477],\n",
      "         [-0.7463]],\n",
      "\n",
      "        [[-1.6040],\n",
      "         [ 1.6029]],\n",
      "\n",
      "        [[-0.8303],\n",
      "         [-0.2477]],\n",
      "\n",
      "        [[-0.2954],\n",
      "         [ 0.8602]]]), 'decoder_target': tensor([[-0.1242, -0.2622],\n",
      "        [-0.4995,  0.3879],\n",
      "        [-0.2854, -0.1242],\n",
      "        [-0.1374,  0.1823]]), 'decoder_lengths': tensor([2, 2, 2, 2]), 'decoder_time_idx': tensor([[7, 8],\n",
      "        [7, 8],\n",
      "        [6, 7],\n",
      "        [7, 8]]), 'groups': tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]]), 'target_scale': tensor([[-0.0557,  0.2767],\n",
      "        [-0.0557,  0.2767],\n",
      "        [-0.0557,  0.2767],\n",
      "        [-0.0557,  0.2767]])}\n",
      "\n",
      "y = (tensor([[-0.1242, -0.2622],\n",
      "        [-0.4995,  0.3879],\n",
      "        [-0.2854, -0.1242],\n",
      "        [-0.1374,  0.1823]]), None)\n",
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([4, 5, 0])\n",
      "\tencoder_cont = torch.Size([4, 5, 1])\n",
      "\tencoder_target = torch.Size([4, 5])\n",
      "\tencoder_lengths = torch.Size([4])\n",
      "\tdecoder_cat = torch.Size([4, 2, 0])\n",
      "\tdecoder_cont = torch.Size([4, 2, 1])\n",
      "\tdecoder_target = torch.Size([4, 2])\n",
      "\tdecoder_lengths = torch.Size([4])\n",
      "\tdecoder_time_idx = torch.Size([4, 2])\n",
      "\tgroups = torch.Size([4, 1])\n",
      "\ttarget_scale = torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagema/anaconda3/envs/csc492_v2/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(dataloader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "  def __init__(self, seq_len, **kwargs):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)\n",
    "    time_linear = self.weights_linear * x + self.bias_linear\n",
    "    time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)\n",
    "\n",
    "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)\n",
    "    return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}